name: Performance Benchmarks

on:
  # Run on releases
  release:
    types: [published]
  # Manual trigger
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save results as new baseline'
        required: false
        default: 'false'
        type: boolean
  # Weekly check
  schedule:
    - cron: '0 10 * * 1'  # Monday 10am UTC

env:
  DEVELOPER_DIR: /Applications/Xcode.app/Contents/Developer

jobs:
  benchmark:
    name: Performance Benchmark
    runs-on: macos-14

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Select Xcode
        run: sudo xcode-select -s /Applications/Xcode.app

      # Install dependencies (customize for your project)
      # - name: Install Dependencies
      #   run: brew install <your-dependencies>

      - name: Cache DerivedData
        uses: actions/cache@v5
        with:
          path: ~/Library/Developer/Xcode/DerivedData
          key: deriveddata-perf-${{ runner.os }}-${{ hashFiles('**/*.swift') }}
          restore-keys: |
            deriveddata-perf-${{ runner.os }}-

      # Measure clean build time
      - name: Clean Build Benchmark
        id: build_time
        run: |
          echo "ðŸ—ï¸ Measuring clean build time..."

          # Clean (customize project/scheme names)
          xcodebuild clean -project YourProject.xcodeproj -scheme YourScheme -quiet 2>/dev/null || true

          # Time the build
          START=$(date +%s)
          xcodebuild build \
            -project YourProject.xcodeproj \
            -scheme YourScheme \
            -destination 'platform=macOS' \
            -quiet
          END=$(date +%s)

          BUILD_TIME=$((END - START))
          echo "build_time=$BUILD_TIME" >> $GITHUB_OUTPUT
          echo "âœ… Build completed in ${BUILD_TIME}s"

      # Run performance tests
      - name: Run Performance Tests
        id: perf_tests
        continue-on-error: true
        run: |
          echo "âš¡ Running performance tests..."

          xcodebuild test \
            -project YourProject.xcodeproj \
            -scheme YourScheme \
            -destination 'platform=macOS' \
            -only-testing:YourProjectTests/PerformanceTests \
            -resultBundlePath PerfResults \
            2>&1 | tee test_output.txt || true

          # Extract performance metrics from output
          grep -E "measured \[.*\] average:" test_output.txt > perf_metrics.txt || true

          if [ -s perf_metrics.txt ]; then
            echo "Found performance metrics:"
            cat perf_metrics.txt
            echo "perf_found=true" >> $GITHUB_OUTPUT
          else
            echo "No performance test metrics found"
            echo "perf_found=false" >> $GITHUB_OUTPUT
          fi

      # Load baseline for comparison
      - name: Load Baseline
        id: baseline
        run: |
          BASELINE_FILE=".claude-team/metrics/perf-baseline.json"

          if [ -f "$BASELINE_FILE" ]; then
            echo "ðŸ“Š Found baseline file"
            BASELINE_BUILD=$(jq -r '.build_time_seconds // 0' "$BASELINE_FILE")
            BASELINE_TEST=$(jq -r '.test_time_seconds // 0' "$BASELINE_FILE")
            echo "baseline_build=$BASELINE_BUILD" >> $GITHUB_OUTPUT
            echo "baseline_test=$BASELINE_TEST" >> $GITHUB_OUTPUT
            echo "has_baseline=true" >> $GITHUB_OUTPUT
          else
            echo "ðŸ“ No baseline found - this will establish initial metrics"
            echo "has_baseline=false" >> $GITHUB_OUTPUT
          fi

      # Check against baseline
      - name: Check Performance Baseline
        if: steps.perf_tests.outputs.perf_found == 'true'
        run: |
          BASELINE_FILE=".claude-team/metrics/perf-baseline.json"

          echo "### Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "$BASELINE_FILE" ]; then
            echo "Comparing against baseline..." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Test | Result |" >> $GITHUB_STEP_SUMMARY
            echo "|------|--------|" >> $GITHUB_STEP_SUMMARY

            # Parse and display metrics
            while IFS= read -r line; do
              TEST_NAME=$(echo "$line" | grep -oE "test[A-Za-z_0-9]+" | head -1)
              AVG_TIME=$(echo "$line" | grep -oE "average: [0-9.]+" | sed 's/average: //')

              if [ -n "$TEST_NAME" ] && [ -n "$AVG_TIME" ]; then
                echo "| $TEST_NAME | ${AVG_TIME}s |" >> $GITHUB_STEP_SUMMARY
              fi
            done < perf_metrics.txt

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Baseline file: $BASELINE_FILE" >> $GITHUB_STEP_SUMMARY
          else
            echo "No baseline file found - establishing initial metrics" >> $GITHUB_STEP_SUMMARY
          fi

      # Measure test execution time
      - name: Test Suite Benchmark
        id: test_time
        run: |
          echo "ðŸ§ª Measuring test execution time..."

          START=$(date +%s)
          xcodebuild test \
            -project YourProject.xcodeproj \
            -scheme YourScheme \
            -destination 'platform=macOS' \
            -skip-testing:YourProjectUITests \
            -quiet 2>&1 || true
          END=$(date +%s)

          TEST_TIME=$((END - START))
          echo "test_time=$TEST_TIME" >> $GITHUB_OUTPUT
          echo "âœ… Tests completed in ${TEST_TIME}s"

      # Regression Detection
      - name: Regression Detection
        id: regression
        run: |
          BUILD_TIME=${{ steps.build_time.outputs.build_time }}
          TEST_TIME=${{ steps.test_time.outputs.test_time }}
          HAS_BASELINE=${{ steps.baseline.outputs.has_baseline }}

          REGRESSION_DETECTED=false
          REGRESSION_DETAILS=""

          if [ "$HAS_BASELINE" == "true" ]; then
            BASELINE_BUILD=${{ steps.baseline.outputs.baseline_build }}
            BASELINE_TEST=${{ steps.baseline.outputs.baseline_test }}

            # Calculate percentage changes
            if [ "$BASELINE_BUILD" -gt 0 ]; then
              BUILD_CHANGE=$(echo "scale=1; (($BUILD_TIME - $BASELINE_BUILD) * 100) / $BASELINE_BUILD" | bc)
              echo "Build time change: ${BUILD_CHANGE}%"

              # Regression threshold: 20% slower
              BUILD_THRESHOLD=20
              if [ "$(echo "$BUILD_CHANGE > $BUILD_THRESHOLD" | bc)" -eq 1 ]; then
                REGRESSION_DETECTED=true
                REGRESSION_DETAILS="Build time regression: ${BUILD_CHANGE}% slower (${BUILD_TIME}s vs ${BASELINE_BUILD}s baseline)"
              fi
            fi

            if [ "$BASELINE_TEST" -gt 0 ]; then
              TEST_CHANGE=$(echo "scale=1; (($TEST_TIME - $BASELINE_TEST) * 100) / $BASELINE_TEST" | bc)
              echo "Test time change: ${TEST_CHANGE}%"

              # Regression threshold: 20% slower
              TEST_THRESHOLD=20
              if [ "$(echo "$TEST_CHANGE > $TEST_THRESHOLD" | bc)" -eq 1 ]; then
                REGRESSION_DETECTED=true
                REGRESSION_DETAILS="${REGRESSION_DETAILS}Test time regression: ${TEST_CHANGE}% slower (${TEST_TIME}s vs ${BASELINE_TEST}s baseline)"
              fi
            fi
          fi

          echo "regression=$REGRESSION_DETECTED" >> $GITHUB_OUTPUT
          echo "details=$REGRESSION_DETAILS" >> $GITHUB_OUTPUT

      # Compare with baseline
      - name: Compare with Baseline
        run: |
          BUILD_TIME=${{ steps.build_time.outputs.build_time }}
          TEST_TIME=${{ steps.test_time.outputs.test_time }}
          HAS_BASELINE=${{ steps.baseline.outputs.has_baseline }}
          REGRESSION=${{ steps.regression.outputs.regression }}

          echo "### âš¡ Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Time | Baseline | Change |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|------|----------|--------|" >> $GITHUB_STEP_SUMMARY

          if [ "$HAS_BASELINE" == "true" ]; then
            BASELINE_BUILD=${{ steps.baseline.outputs.baseline_build }}
            BASELINE_TEST=${{ steps.baseline.outputs.baseline_test }}

            BUILD_DIFF=$((BUILD_TIME - BASELINE_BUILD))
            TEST_DIFF=$((TEST_TIME - BASELINE_TEST))

            if [ "$BUILD_DIFF" -gt 0 ]; then
              BUILD_STATUS="âš ï¸ +${BUILD_DIFF}s"
            elif [ "$BUILD_DIFF" -lt 0 ]; then
              BUILD_STATUS="âœ… ${BUILD_DIFF}s"
            else
              BUILD_STATUS="âœ… no change"
            fi

            if [ "$TEST_DIFF" -gt 0 ]; then
              TEST_STATUS="âš ï¸ +${TEST_DIFF}s"
            elif [ "$TEST_DIFF" -lt 0 ]; then
              TEST_STATUS="âœ… ${TEST_DIFF}s"
            else
              TEST_STATUS="âœ… no change"
            fi

            echo "| Clean Build | ${BUILD_TIME}s | ${BASELINE_BUILD}s | $BUILD_STATUS |" >> $GITHUB_STEP_SUMMARY
            echo "| Test Suite | ${TEST_TIME}s | ${BASELINE_TEST}s | $TEST_STATUS |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Clean Build | ${BUILD_TIME}s | - | ðŸ“ first measurement |" >> $GITHUB_STEP_SUMMARY
            echo "| Test Suite | ${TEST_TIME}s | - | ðŸ“ first measurement |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # Regression warnings
          if [ "$REGRESSION" == "true" ]; then
            echo "### âš ï¸ Performance Regression Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Performance has degraded by more than 20% compared to baseline." >> $GITHUB_STEP_SUMMARY
            echo "Consider investigating before merging." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Absolute thresholds (fallback)
          if [ "$BUILD_TIME" -gt 120 ]; then
            echo "âš ï¸ **Warning:** Build time exceeds 2 minutes" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "$TEST_TIME" -gt 60 ]; then
            echo "âš ï¸ **Warning:** Test time exceeds 1 minute" >> $GITHUB_STEP_SUMMARY
          fi

      # Save metrics artifact
      - name: Save Metrics
        run: |
          mkdir -p metrics
          cat > metrics/benchmark-$(date +%Y%m%d).json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "version": "$(cat VERSION.txt | tr -d '[:space:]')",
            "commit": "${{ github.sha }}",
            "build_time_seconds": ${{ steps.build_time.outputs.build_time }},
            "test_time_seconds": ${{ steps.test_time.outputs.test_time }},
            "regression_detected": ${{ steps.regression.outputs.regression }}
          }
          EOF

      # Save as new baseline if requested
      - name: Save Baseline
        if: inputs.save_baseline == 'true' || (github.event_name == 'release')
        run: |
          echo "ðŸ“Š Saving new performance baseline..."
          mkdir -p .claude-team/metrics
          cat > .claude-team/metrics/perf-baseline.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "version": "$(cat VERSION.txt | tr -d '[:space:]')",
            "commit": "${{ github.sha }}",
            "build_time_seconds": ${{ steps.build_time.outputs.build_time }},
            "test_time_seconds": ${{ steps.test_time.outputs.test_time }}
          }
          EOF
          echo "âœ… Baseline saved to .claude-team/metrics/perf-baseline.json"

      - name: Upload Metrics
        uses: actions/upload-artifact@v6
        with:
          name: performance-metrics
          path: metrics/
          retention-days: 90

      # Fail on regression (optional - uncomment to make blocking)
      # - name: Fail on Regression
      #   if: steps.regression.outputs.regression == 'true'
      #   run: |
      #     echo "âŒ Performance regression detected!"
      #     echo "${{ steps.regression.outputs.details }}"
      #     exit 1
