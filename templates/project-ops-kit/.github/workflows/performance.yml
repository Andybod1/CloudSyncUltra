name: Performance Benchmarks

on:
  # Run on releases
  release:
    types: [published]
  # Manual trigger
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save results as new baseline'
        required: false
        default: 'false'
        type: boolean
  # Weekly check
  schedule:
    - cron: '0 10 * * 1'  # Monday 10am UTC

env:
  DEVELOPER_DIR: /Applications/Xcode.app/Contents/Developer

jobs:
  benchmark:
    name: Performance Benchmark
    runs-on: macos-14

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Select Xcode
        run: sudo xcode-select -s /Applications/Xcode.app

      - name: Install rclone
        run: brew install rclone

      - name: Cache DerivedData
        uses: actions/cache@v5
        with:
          path: ~/Library/Developer/Xcode/DerivedData
          key: deriveddata-perf-${{ runner.os }}-${{ hashFiles('**/*.swift') }}
          restore-keys: |
            deriveddata-perf-${{ runner.os }}-

      # Measure clean build time
      - name: Clean Build Benchmark
        id: build_time
        run: |
          echo "ðŸ—ï¸ Measuring clean build time..."

          # Clean
          xcodebuild clean -project CloudSyncApp.xcodeproj -scheme CloudSyncApp -quiet 2>/dev/null || true

          # Time the build
          START=$(date +%s)
          xcodebuild build \
            -project CloudSyncApp.xcodeproj \
            -scheme CloudSyncApp \
            -destination 'platform=macOS' \
            -quiet
          END=$(date +%s)

          BUILD_TIME=$((END - START))
          echo "build_time=$BUILD_TIME" >> $GITHUB_OUTPUT
          echo "âœ… Build completed in ${BUILD_TIME}s"

      # Run performance tests
      - name: Run Performance Tests
        id: perf_tests
        continue-on-error: true
        run: |
          echo "âš¡ Running performance tests..."

          xcodebuild test \
            -project CloudSyncApp.xcodeproj \
            -scheme CloudSyncApp \
            -destination 'platform=macOS' \
            -only-testing:CloudSyncAppTests/PerformanceTests \
            -resultBundlePath PerfResults \
            2>&1 | tee test_output.txt || true

          # Extract performance metrics from output
          grep -E "measured \[.*\] average:" test_output.txt > perf_metrics.txt || true

          if [ -s perf_metrics.txt ]; then
            echo "Found performance metrics:"
            cat perf_metrics.txt
            echo "perf_found=true" >> $GITHUB_OUTPUT
          else
            echo "No performance test metrics found"
            echo "perf_found=false" >> $GITHUB_OUTPUT
          fi

      # Check against baseline
      - name: Check Performance Baseline
        if: steps.perf_tests.outputs.perf_found == 'true'
        run: |
          BASELINE_FILE=".claude-team/metrics/perf-baseline.json"

          echo "### Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "$BASELINE_FILE" ]; then
            echo "Comparing against baseline..." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Test | Result |" >> $GITHUB_STEP_SUMMARY
            echo "|------|--------|" >> $GITHUB_STEP_SUMMARY

            # Parse and display metrics
            while IFS= read -r line; do
              TEST_NAME=$(echo "$line" | grep -oE "test[A-Za-z_0-9]+" | head -1)
              AVG_TIME=$(echo "$line" | grep -oE "average: [0-9.]+" | sed 's/average: //')

              if [ -n "$TEST_NAME" ] && [ -n "$AVG_TIME" ]; then
                echo "| $TEST_NAME | ${AVG_TIME}s |" >> $GITHUB_STEP_SUMMARY
              fi
            done < perf_metrics.txt

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Baseline file: $BASELINE_FILE" >> $GITHUB_STEP_SUMMARY
          else
            echo "No baseline file found - establishing initial metrics" >> $GITHUB_STEP_SUMMARY
          fi

      # Measure test execution time
      - name: Test Suite Benchmark
        id: test_time
        run: |
          echo "ðŸ§ª Measuring test execution time..."

          START=$(date +%s)
          xcodebuild test \
            -project CloudSyncApp.xcodeproj \
            -scheme CloudSyncApp \
            -destination 'platform=macOS' \
            -skip-testing:CloudSyncAppUITests \
            -quiet 2>&1 || true
          END=$(date +%s)

          TEST_TIME=$((END - START))
          echo "test_time=$TEST_TIME" >> $GITHUB_OUTPUT
          echo "âœ… Tests completed in ${TEST_TIME}s"

      # Compare with baseline
      - name: Compare with Baseline
        run: |
          BUILD_TIME=${{ steps.build_time.outputs.build_time }}
          TEST_TIME=${{ steps.test_time.outputs.test_time }}

          echo "### âš¡ Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Time |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|------|" >> $GITHUB_STEP_SUMMARY
          echo "| Clean Build | ${BUILD_TIME}s |" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | ${TEST_TIME}s |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check for regressions (>20% slower than typical)
          if [ "$BUILD_TIME" -gt 120 ]; then
            echo "âš ï¸ **Warning:** Build time exceeds 2 minutes" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "$TEST_TIME" -gt 60 ]; then
            echo "âš ï¸ **Warning:** Test time exceeds 1 minute" >> $GITHUB_STEP_SUMMARY
          fi

      # Save metrics artifact
      - name: Save Metrics
        run: |
          mkdir -p metrics
          cat > metrics/benchmark-$(date +%Y%m%d).json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "version": "$(cat VERSION.txt | tr -d '[:space:]')",
            "commit": "${{ github.sha }}",
            "build_time_seconds": ${{ steps.build_time.outputs.build_time }},
            "test_time_seconds": ${{ steps.test_time.outputs.test_time }}
          }
          EOF

      - name: Upload Metrics
        uses: actions/upload-artifact@v6
        with:
          name: performance-metrics
          path: metrics/
          retention-days: 90
